{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toml\n",
    "import cohere  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "\n",
    "# Load the secret.toml file\n",
    "with open('secret.toml') as f:\n",
    "    secrets = toml.load(f)\n",
    "\n",
    "# Access the API key value\n",
    "api_key = secrets['API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cohere_text_preprocessing.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add an id column\n",
    "df['id'] = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(df, width=1500, overlap=500):\n",
    "    # create an empty dataframe to store the chunked text\n",
    "    new_df = pd.DataFrame(columns=['id', 'text_chunk'])\n",
    "\n",
    "    # iterate over each row in the original dataframe\n",
    "    for index, row in df.iterrows():\n",
    "        # split the text into chunks of size 'width', with overlap of 'overlap'\n",
    "        chunks = []\n",
    "        for i in range(0, len(row['text']), width - overlap):\n",
    "            chunk = row['text'][i:i+width]\n",
    "            chunks.append(chunk)\n",
    "\n",
    "        # iterate over each chunk and add it to the new dataframe\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # calculate the start index based on the chunk index and overlap\n",
    "            start_index = i * (width - overlap)\n",
    "            \n",
    "            # create a new row with the chunked text and the original row's ID\n",
    "            new_row = {'id': row['id'], 'text_chunk': chunk, 'start_index': start_index}\n",
    "            new_df = new_df.append(new_row, ignore_index=True)\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "\n",
    "new_df = chunk_text(df)\n",
    "# append text chunks to the original dataframe in id order\n",
    "df = df.merge(new_df, on='id', how='left')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "co = cohere.Client(api_key) \n",
    "\n",
    "# Get the embeddings\n",
    "embeds = co.embed(texts=list(df['text_chunk']),\n",
    "                  model=\"large\",\n",
    "                  truncate=\"RIGHT\").embeddings\n",
    "# Check the dimensions of the embeddings\n",
    "embeds = np.array(embeds)\n",
    "embeds.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the search index, pass the size of embedding\n",
    "search_index = AnnoyIndex(embeds.shape[1], 'angular')\n",
    "# Add all the vectors to the search index\n",
    "for i in range(len(embeds)):\n",
    "    search_index.add_item(i, embeds[i])\n",
    "\n",
    "search_index.build(10) # 10 trees\n",
    "search_index.save('search_index.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the dataframe to a csv file\n",
    "df.to_csv('cohere_text_final.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
